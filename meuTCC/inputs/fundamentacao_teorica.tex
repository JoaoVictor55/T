\chapter{Fundamentação Teórica}

\section{Funcionamento do coração}
\label{sec:funciionamento_coracao}

O coração é um órgão muscular composto por quatro câmaras — átrio direito e esquerdo, e ventrículo direito e esquerdo — que se contraem de forma rítmica, bombeando sangue para o corpo. Essas contrações são controladas por correntes elétricas que percorrem o coração de maneira precisa e em velocidade controlada.

Na figura \ref{fig:coracao_esquema_eletrico}, o sistema de condução elétrico do coração é ilustrado.

\begin{figure}[H]
  \centering
  \caption{Sistema de condução do coração}
   \includegraphics[width=0.6\textwidth]{figuras/coracao_sistema_eletrico.png} % insere o tikzpicture puro
  \label{fig:coracao_esquema_eletrico}
    \legend{Fonte: Adaptado de \citeonline{Mitchell_Arritmias_MSD}}
\end{figure}

Segundo \citeonline{Mitchell_Arritmias_MSD}, o batimento cardíaco normal se inicia no nódulo sinusal (1), localizado no átrio direito, que atua como o marcapasso natural do coração. A corrente elétrica propaga-se do átrio direito para o esquerdo (2), promovendo sua contração e o bombeamento do sangue para os ventrículos. Em seguida, o impulso atinge o nódulo atrioventricular (3) — conexão entre os átrios e ventrículos — onde é temporariamente retardado, permitindo que os átrios se contraiam completamente e encham as câmaras inferiores.

Posteriormente, a corrente percorre o feixe de His (4), que se divide e conduz o impulso para ambos os ventrículos (5), promovendo sua contração e o bombeamento do sangue para o restante do corpo.

\section{O eletrocardiograma}
\label{sec:ecg}

O eletrocardiograma, ECG, é um enxame não invasivo usado para medir a atividade elétrica do coração. Ele é feito a partir do contato de eletrodos, chamados de derivações ou \textit{leads}, sobre a pele.
A quantidade de eletrodos varia, mas geralmente são 12 \cite{msd_ecg}.

Ao registrar a magnitude e direção da corrente, as derivações geram uma onda que representa a atividade elétrica do coração. O passo a passo descrito em \ref{sec:funciionamento_coracao} é refletido em sua morfologia.

Na figura \ref{fig:ecg_exemplo_coracao}, é ilustrado um  ECG de um batimento, observe que ele é subdividido em: onda P, complexo QRS e onda T \cite{msd_ecg}. Note que cada uma dessas partes se refere a um estágio do batimento.

\begin{figure}[H]
  \centering
  \caption{Exemplo de ECG com sua morfologia destacada}
   \includegraphics[width=0.8\textwidth]{figuras/ecg_exemplo_coracao.png} % insere o tikzpicture puro
  \label{fig:ecg_exemplo_coracao}
    \legend{Fonte: Adaptado de  \citeonline{msd_ecg}}
\end{figure}

Dentre as doenças que podem ser diagnósticas via ECG estão as arritmias cardíacas. 

\section{Arritmias}


\subsection{Arritmias clínicas}

As arritmias são alterações no ritmo cardíaco que podem ter diversas causas, incluindo alterações hormonais, uso de medicamentos, toxinas (como álcool ou cafeína), anomalias eletrolíticas ou doenças cardíacas.
Segundo \citeonline{Mitchell_Arritmias_MSD}, em adultos em repouso, a frequência cardíaca normal varia entre 60 e 100 batimentos por minuto (bpm). Frequências mais baixas, conhecidas como bradicardia sinusal, são comuns em atletas, crianças pequenas, adolescentes, jovens adultos e durante o sono. Por outro lado, a taquicardia sinusal ocorre quando a frequência se eleva, podendo ser observada durante o esforço físico, doenças, estimulação neural simpática ou emoção intensa.

Variações no ritmo cardíaco são fenômenos fisiológicos normais. Durante a respiração, por exemplo, é comum que a frequência aumente e diminua levemente — comportamento conhecido como arritmia sinusal respiratória. O autor ainda observa que um ritmo cardíaco perfeitamente regular pode indicar patologias no sistema nervoso autônomo, como ocorre em casos de diabetes avançado. Dessa forma, ainda não existe um indicador global e definitivo do que seria um ritmo sinusal considerado saudável.
As arritmias podem ser classificadas de forma simplificada em três tipos principais:

\begin{enumerate}
    \item Taquicardia: frequência excessivamente rápida;
    \item Bradicardia: frequência excessivamente lenta;
    \item Irregular: quando os impulsos percorrem o coração por vias irregulares.
\end{enumerate}

Observe na figura \ref{fig:ecg_exemplo_coracao}, exemplos desses três tipos arrítmicos ilustrados em um ECG.

\subsection{Padrões de classificação para algoritmos (AAMI)}
\label{sub_sec:padroes_arritmias_aami}

A AAMI define cinco classes de arritmia: normal (N), ventricular (V), supraventricular (S), fusão (F) e não classificado (Q) \cite{silva2025,saadatnejad2020}.

As arritmias ventriculares incluem, por exemplo, as contrações prematuras ventriculares (PVCs) e os batimentos de escape. 
O batimento ventricular de escape atua como um mecanismo compensatório, funcionando como um “backup” protetivo do coração quando o marcapasso natural falha temporariamente.
De acordo com \citeonline{Sattar_Premature_2025}, as PVCs são batimentos originários dos ventrículos que podem ocorrer mesmo em indivíduos saudáveis. Sua morfologia é variável, dependendo da origem do impulso, de doenças estruturais ou ainda de uso de medicamentos. Quando frequentes, podem causar fadiga e palpitações, evoluindo para disfunções ventriculares e, em alguns casos, representando a primeira manifestação de cardiopatias estruturais.

\citeonline{msdmanuals_ventricular} cita ainda outras causas potenciais de PVCs, como doenças da artéria coronária (especialmente durante ou após infarto), dilatação ventricular decorrente de insuficiência cardíaca e alterações nas válvulas cardíacas. Em pacientes com doenças estruturais, as PVCs podem progredir para arritmias mais graves, como taquicardia ventricular (VT) e fibrilação ventricular (FV), que representam risco de morte súbita.

\citeonline{mitchell2024afib,mitchell2024vt} descrevem tanto a fibrilação quanto a taquicardia ventricular como arritmias originadas nos ventrículos.

A taquicardia ventricular (TV) produz uma frequência cardíaca de até 120 bpm e é formada por uma sequência de contrações ventriculares prematuras (PVCs). Quando persiste por mais de 30 segundos, recebe a denominação de taquicardia sustentada. Costuma ocorrer em indivíduos com alterações estruturais cardíacas, como infarto do miocárdio, falha cardíaca ou cardiomiopatia. Os sintomas incluem fraqueza, tontura e desconforto torácico. Caso persista por mais de 30 segundos, o tratamento é indicado mesmo na ausência de sintomas, uma vez que pode evoluir para fibrilação ventricular.

Outro tipo de arritmia ventricular é o \textit{flutter} ventricular. Segundo o \citeonline{mesh_ventricular_flutter}, elas são caracterizadas por uma taquicardia extremamente rápida e instável hemodinamicamente (150 a 300 bpm).
São potencialmente fatais e, tipicamente, evoluem para a fibrilação ventricular.

A fibrilação ventricular (FV), por outro lado, é caracterizada por batimentos rápidos e desordenados, resultantes de sinais elétricos caóticos nos ventrículos. Essa condição leva à perda de consciência em poucos segundos e à morte caso não haja intervenção imediata, configurando-se como um tipo de parada cardíaca. Entre suas causas estão afogamentos, choques elétricos e falha cardíaca.

Apesar de ocorrerem também em indivíduos saudáveis, as PVCs possuem significância clínica, pois podem estar associadas a outros tipos de arritmias ventriculares graves. É importante considerar o contexto do batimento: no caso da taquicardia ventricular, por exemplo, o diagnóstico é estabelecido a partir de uma sequência de PVCs que produz uma frequência cardíaca elevada (>20 bpm). Assim, o diagnóstico depende de uma combinação de características temporais e morfológicas, observando-se o contexto dos batimentos e, naturalmente, os sintomas clínicos.

Dentro da classe dos batimentos normais, além do batimento típico descrito na \ref{sec:funciionamento_coracao}, incluem-se também os batimentos atriais de escape e os bloqueios do ramo esquerdo e direito. Estes últimos, embora inofensivos por si só, podem indicar condições cardíacas subjacentes mais graves, como doença da artéria coronária ou infarto do miocárdio prévio \cite{mitchell2024hisbloqueio}.

Segundo o Texas Heart Institute \cite{texasheart_arrhythmias}, arritmias superventricular se originam acima dos ventrículos, como nos 
átrios ou nos caminhos de condução. Geralmente, são mais benignas que as arritmias ventriculares podendo ocorrer, assim como os PVCs, em 
resposta ao consumo de cafeína, tabaco, álcool, tosse ou remédio para resfriado. Outras causas incluem problemas na tireoide. Elas podem causar
palpitações, falta de ar ou aperto no peito.

As contrações superventriculares prematuras, por exemplo, ocorrem quando o átrio se contrai muito cedo. A fibrilação atrial, por outro lado,
são batimentos rápidos e irregulares, causadas por contrações desordenadas das fibras musculares. São as principais causas de 
AVC em idosos, pois causam o acúmulo de sangue nos átrios que podem coagular e viajar até o cérebro. 

\citeonline{statpearls_svt} descreve outros tipos de arritmias superventriculares, como as traquicardias superventricular que são formadas 
por desordens rítmicas rápidas, taquicardia, que são caracterizadas por um complexo QRS mais estreito, menor que 120 ms, e uma 
frequência cardíaca alta. Em adultos, ela é superior a 100 bpm enquanto que em crianças, varia de 180 à 220 bpm. 

Os autores apontam como as traquicardias superventriculares mais comuns a taquicardia atrioventricular nodal reentrante e a taquicardia
atrioventricular recíproca. 

Portanto, as arritmias podem ser classificadas de acordo com sua origem — ventriculares, quando se iniciam nos ventrículos, ou supraventriculares, quando se originam acima deles, como nos átrios.
Essas alterações podem se manifestar por um ritmo cardíaco acelerado ou retardado, ou ainda por uma condução elétrica anormal, o que se reflete na morfologia do traçado eletrocardiográfico (ECG).

Não há, entretanto, padrões universais que definam o que é uma atividade cardíaca normal, pois o ECG apresenta variações fisiológicas relacionadas, por exemplo, à faixa etária, nível de atividade física ou condições clínicas individuais. Diversos fatores podem influenciar a forma do sinal e gerar anomalias que nem sempre têm significado patológico.

Por fim, a diversidade das arritmias também se expressa dentro de uma mesma classe. No caso das arritmias ventriculares, por exemplo, é possível observar desde contrações ventriculares prematuras (PVCs) até taquicardias ventriculares (TVs), ilustrando a amplitude de manifestações possíveis.

\section{Redes Neurais Artificiais}
\label{sec:ann}

Segundo \citeonline{geron2022hands}, as redes neurais artificiais (ANN) foram introduzidas em 1943 pelo neurofisiologista Warren McCulloch e pelo matemático Walter Pitts como um modelo simplificado de como os neurônios biológicos podem realizar cálculos complexos por meio de lógica proposicional.

O sucesso inicial levou à crença de que tais modelos poderiam ser usados no desenvolvimento de sistemas verdadeiramente inteligentes. No entanto, em parte devido às limitações na capacidade de processamento e à escassez de dados disponíveis, as ANNs foram gradualmente abandonadas em favor de outras técnicas, como as máquinas de vetor de suporte (\textit{support vector machines} — SVMs).

Nos últimos anos, entretanto, com a criação de grandes conjuntos de dados, o aumento do poder computacional proporcionado pelas GPUs e o desenvolvimento de novas arquiteturas, as ANNs passaram por um reavivamento, consolidando-se como uma das técnicas mais promissoras da inteligência artificial.

O autor descreve os neurônios biológicos como células formadas por um corpo celular e uma longa extensão chamada axônio. A partir do corpo celular, partem diversas ramificações denominadas dendritos. Na extremidade do axônio, ocorrem ramificações menores chamadas telodendriais, cujas pontas possuem estruturas denominadas terminais sinápticos.

O axônio de um neurônio conecta-se ao dendrito de outro, formando uma sinapse — o ponto de comunicação entre as células.

Esses neurônios transmitem sinais elétricos chamados potenciais de ação (\textit{action potentials} — APs), que percorrem o axônio até os terminais sinápticos, onde estimulam a liberação de substâncias químicas conhecidas como neurotransmissores. Esses neurotransmissores permitem a comunicação entre os neurônios, podendo estimular ou inibir o disparo de novos potenciais de ação.

Bilhões de neurônios interligam-se por meio de sinapses, formando redes altamente complexas. A ação coordenada dessas redes possibilita a realização de cálculos e processos cognitivos sofisticados, que inspiraram o desenvolvimento das redes neurais artificiais.

Esse é o princípio em que se baseiam as redes neurais artificiais (ANNs): pequenas unidades computacionais simples que, ao se conectarem, são capazes de realizar funções complexas. É importante destacar que um neurônio artificial, no contexto das ANNs, não é uma réplica de um neurônio biológico, mas sim uma abstração matemática inspirada em seu funcionamento.

Segundo \citeonline{james2023}, uma ANN pode ser vista como uma função que recebe um conjunto de  p preditores e, por meio de transformações não lineares, busca prever uma variável resposta Y.
Na Figura \ref{fig:ann_ff_exemplo}, é ilustrada uma arquitetura simples de rede neural feedforward.

\begin{figure}[H]
  \centering
  \caption{Exemplo de uma ANN \textit{feed-foward}}
   \includegraphics[width=0.8\textwidth]{figuras/ann_exemplos/ann_ff_exemplo.png} % insere o tikzpicture puro
  \label{fig:ann_ff_exemplo}
    \legend{Fonte: Adaptado de  \citeonline{james2023}}
\end{figure}

Essa rede é composta por três camadas principais: a camada de entrada (input layer), a camada oculta (hidden layer) e a camada de saída (output layer).
Cada neurônio na camada oculta é conectado a todos os neurônios da camada de entrada, assim como os neurônios da camada de saída são conectados aos da camada oculta.

A cada conexão é atribuído um peso, que representa a força da relação entre os neurônios — em analogia à intensidade das sinapses no cérebro biológico. Além disso, cada neurônio possui um viés (bias), que desloca o ponto de ativação.

A ativação de um neurônio é determinada por uma função de ativação não linear, essencial para permitir que a rede aprenda relações complexas entre as variáveis de entrada.
Sem essa função, uma ANN de múltiplas camadas seria equivalente a uma simples regressão linear.

Formalmente, uma ANN pode ser descrita como uma função  f(X) que recebe um vetor de p preditores X=(X1,X2,…,Xp) e produz uma previsão para a variável resposta Y.

No caso da rede ilustrada na Figura \ref{fig:ann_ff_exemplo}, essa função  f pode ser expressa como:

\begin{equation}
f(X) = \beta_0 + \sum_{k=1}^{K} \beta_k h_k(X)
\end{equation}

Onde K refere-se a quantidade de neurônios na camada oculta. \textit{h} é expandido em:

\begin{equation}
h_k = g(w_{k0} + \sum_{j=1}^{p}w_{kj}X_j)
\end{equation}

Assim, \textit{f} é construída em dois passos; primeiro, K ativações da camada oculta são calculados:

\begin{equation}
  A_k = h_k(X) = g(w_{k0} + \sum_{j=1}^{p}w_{kj}X_k)
\end{equation}

Onde \( g(z) \) é a função de ativação não linear. 
Os parâmetros \( \beta_0, \dots, \beta_K \) e \( \omega_0, \dots, \omega_{Kp} \) precisam ser estimados — isto é, aprendidos a partir dos dados.

Existem diversas funções de ativação propostas ao longo do tempo. 
A sigmoide, por exemplo, foi amplamente utilizada nas primeiras redes neurais:

\begin{equation}
  g(z) = \frac{e^z}{1 + e^z} = \frac{1}{1 + e^{-z}}
\end{equation}

Segundo \citeonline{geron2022hands}, essa função é uma aproximação do comportamento de ativação observado em neurônios biológicos. 
Entretanto, atualmente ela é pouco utilizada nas camadas intermediárias, uma vez que é propensa ao problema do \textit{vanishing gradient} e não é centrada em zero, o que prejudica a convergência do modelo.  
Apesar disso, a sigmoide ainda é frequentemente empregada na camada de saída em problemas de classificação binária, pois seu contradomínio é [0, 1]; o que
torna simples interpretar a saída dos neurônios como probabilidades.

Outra função comumente utilizada é a tangente hiperbólica:

\begin{equation}
  g(z) = \tanh(z) = 2\sigma(2z) - 1
\end{equation}

Essa função é centrada em zero e possui uma derivada mais acentuada, o que ajuda a mitigar o problema do \textit{vanishing gradient}.

Atualmente, a função de ativação mais popular é a ReLU (\textit{Rectified Linear Unit}), definida como:

\begin{equation}
g(z) = z_+ =
\begin{cases}
0, & \text{se } z < 0 \\
z, & \text{caso contrário}
\end{cases}
\end{equation}

A ReLU apresenta um cálculo simples e eficiente, além de acelerar o treinamento de redes profundas. 
Embora não seja centrada em zero, o viés dos neurônios auxilia a compensar esse deslocamento.  
Na Figura \ref{fig:funcoes_ativacao_exemplo}, são ilustradas as três funções de ativação discutidas.

\begin{figure}[H]
  \centering
  \caption{Funções de ativação Sigmoid, Tanh e ReLU}
  \includegraphics[width=0.8\textwidth]{figuras/ann_exemplos/funcoes_de_ativacao.png}
  \label{fig:funcoes_ativacao_exemplo}
  \legend{Fonte: Elaborado pelo autor.}
\end{figure}

\subsection{Treinamento de redes neurais}

ANNs aprendem através da minimização de uma função de custo. Uma função muito utilizada para problemas de classificação binária
é a \textit{binary cross-entropy}:

\begin{equation}
-\frac{1}{N}\sum_{i=1}^{N}[y_i\log(p_i) + (1 - y_i)\log(1 - p_i)]
\label{for:cross_entropy}
\end{equation}

Onde \( N \) é a quantidade de amostras.

Como já discutido, na camada de saída costuma-se utilizar a função de ativação sigmoide.
Assim, uma interpretação para a Equação \ref{for:cross_entropy} é que ela penaliza o modelo quando a probabilidade estimada para uma amostra 
se distancia de sua classe real. Por exemplo, considere que uma amostra pertença à classe positiva (\(y = 1\)); se o modelo prevê uma 
probabilidade de apenas 0,01, o erro será significativamente maior do que se tivesse previsto 0,6.

Para minimizar essa função de custo, redes neurais artificiais utilizam algoritmos baseados no \textit{gradiente descendente}. 
Segundo \citeonline{geron2022hands}, o gradiente descendente busca encontrar o mínimo de uma função ajustando iterativamente seus parâmetros. 
Intuitivamente, a função de custo define uma superfície de erro, e o modelo inicia em um ponto aleatório dessa superfície — 
isto é, com pesos inicializados aleatoriamente. 

A cada iteração, o gradiente da função é calculado no ponto atual, indicando a direção de maior crescimento da função de custo. 
Os parâmetros são então atualizados na direção oposta, ou seja, na direção de maior decaimento. 
O tamanho desse passo é controlado por um hiperparâmetro chamado taxa de aprendizado (\textit{learning rate}), denotado por \( \eta \). 
Formalmente, a atualização é dada por:

\begin{equation}
\theta^{(i+1)} = \theta^{(i)} - \eta \nabla f(\theta)
\label{for:gradiente_descendente}
\end{equation}

onde \( \theta \) representa o vetor de parâmetros do modelo (pesos e vieses), e \( \nabla f(\theta) \) é o gradiente da função de custo 
em relação a esses parâmetros.

Na prática, utiliza-se variações do algoritmo, como o \textit{stochastic gradient descent} (SGD), que atualiza os parâmetros a partir de 
amostras individuais ou pequenos subconjuntos (\textit{mini-batches}) dos dados, e otimizadores mais sofisticados como o Adam, que ajusta 
automaticamente a taxa de aprendizado ao longo do treinamento.

O processo de calculo dos gradientes é realizado por outro algoritmo, chamado de \textit{backpropagation}. 
Segundo \citeonline{geron2022hands}, um dos principais obstáculos históricos para o uso eficiente de redes neurais artificiais 
era a dificuldade de ajustar seus parâmetros quando a rede possuía muitas camadas. 
Aprofundar a rede, entretanto, é essencial para que ela aprenda representações (\textit{features}) mais complexas dos dados.

Em 1985, David Rumelhart, Geoffrey Hinton e Ronald Williams demonstraram que um algoritmo proposto anteriormente por Seppo Linnainmaa, em 1970, 
poderia ser aplicado para esse fim. Esse algoritmo, atualmente conhecido como \textit{backpropagation}, tornou possível o treinamento eficiente 
de redes com múltiplas camadas.

De forma simplificada, o \textit{backpropagation} é composto por duas etapas principais: \textit{forward pass} e \textit{backward pass}.

Na fase \textit{forward}, os dados de entrada percorrem a rede camada por camada. 
Cada neurônio realiza seu cálculo, multiplicando os valores de entrada pelos pesos correspondentes, somando o viés e aplicando a função de ativação. 
O resultado dessa sequência de operações é a saída da rede, que é então comparada com o valor real (\textit{ground truth}) 
por meio de uma função de perda, que mede o erro do modelo.

Na fase \textit{backward}, o algoritmo utiliza a regra da cadeia do cálculo diferencial para determinar quanto cada parâmetro 
(peso e viés) contribuiu para o erro total. 
Esse cálculo é feito de trás para frente, da camada de saída até a camada de entrada, ajustando os gradientes de forma eficiente. 
Dessa maneira, cada peso recebe uma medida precisa de sua influência no erro observado.

Por fim, os parâmetros são atualizados aplicando-se o gradiente descendente — ou outra técnica de otimização —, 
movendo os pesos na direção que reduz o erro da rede.

\section{Redes Neurais Convolucionais}

\section{Redes Neurais Recorrentes e suas variações}

Uma rede neural recorrente — RNN — pode ser entendida intuitivamente como uma rede com uma memória. \citeonline{james2023} diz que as RNN
se destacam em dados cuja natureza é sequencial, tais como:

\begin{itemize}
    \item Documentos tais como resenhas de livros, filmes, jornais e \textit{tweets}. O autor explica que a sequência e posição relativa
    das palavras ajuda a capturar a narrativa, os temas e os tons. Dentre as aplicações, é possível citar analise de sentimento e tradução.
    \item Series temporais como temperatura, precipitação, velocidade do vento, qualidade do ar e outras. Assim, é possível prever o clima 
    com uma janela de dias ou mesmo décadas.
    \item Sinais sonoros como voz gravada, música e outros. Uma das aplicações citadas é legendas automáticas, tradução ou mesmo, a classificação
    de determinadas características.
\end{itemize}

A entrada de uma RNN é uma sequência. Por exemplo, se a tarefa for classificar um documento, um documento seria uma sequência \(X = \{X_1, X_2, ..., X_L\}\) de 
\textit{L} palavras onde cada \textit{\(X_{l}\)} representa uma palavra. Na figura \ref{fig:rnn_exemplo} é ilustrada uma simples RNN com apenas um neurônio na camada
de entrada, oculta e saída.

\begin{figure}[H]
  \centering
  \caption{Exemplo de uma RNN}
   \includegraphics[width=0.8\textwidth]{figuras/ann_exemplos/rnn_exemplo.png} % insere o tikzpicture puro
  \label{fig:rnn_exemplo}
    \legend{Fonte: Adaptado de  \citeonline{james2023}}
\end{figure}

O \textit{loop} na parte da esquerda simboliza a retro-alimentação da rede. Na direita, a RNN é "desenrolada" assim: 
a rede processa um \(X_l\) de cada vez e produz a ativação \(A_{l}\) a partir de tanto dessa entrada quanto da ativação
\(A_{l-1}\), ou seja, a ativação anterior. \(A_{l}\) é utilizado como entrada da camada de saída, produzindo \(O_{l}\).
Neste exemplo, apenas a última ativação desse camada é utilizada como saída final.

Mais formalmente, supondo que \(X_l\) tenha \textit{p} componentes; \(X_{l}^{T} = (X_{l1}, X_{l2}, ..., X_{lp}) \)
e a camada de saída tenha \textit{K} unidades \(A_{l}^{T} = (A_{l1}, A_{l2}, ..., A_{lK})\). A matriz \textbf{W} 
é uma coleção de \(K \times (p + 1)\) pesos da camada de entrada e \textbf{U} é uma coleção de \(K \times K\) pesos 
da camada recorrente e \textbf{B} é um vetor de \(K + 1\) pesos para a camada de saída. A ativação é calculado como:

\begin{equation}
  A_{lk} = g\left( w_{k0}+ \sum_{j=1}^{p}w_{kj}\times X_{l,p}  + \sum_{s=1}^Ku_{ks}\times A_{l-1,s}\right)
\end{equation}