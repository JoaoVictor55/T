\chapter{Fundamentação Teórica}
\label{ch:fundamentacao}

Como o trabalho envolve o reconhecimento de arritmias cardíacas, o primeiro ponto é compreender
como o coração funciona, mais especialmente, como o funciona o mecanismo do impulso elétrico responsáveis pela 
contração do mesmo. Em seguida, entender o princípio de funcionamento das redes neurais, que integra a solução 
para o problema. Por fim, será apresentado alguns trabalhos que correlacionaram as duas áreas.

\section{Funcionamento do coração}
\label{sec:funciionamento_coracao}

O coração é um órgão muscular composto por quatro câmaras — átrio direito e esquerdo, e ventrículo direito e esquerdo — que se contraem de forma rítmica, bombeando sangue para o corpo. Essas contrações são controladas por correntes elétricas que percorrem o coração de maneira precisa e em velocidade controlada.

Na Figura~\ref{fig:coracao_esquema_eletrico}, o sistema de condução elétrica do coração é ilustrado.

\begin{figure}[H]
  \centering
  \caption{Sistema de condução do coração}
   \includegraphics[width=0.35\textwidth]{figuras/coracao_sistema_eletrico.png} % insere o tikzpicture puro
  \label{fig:coracao_esquema_eletrico}
    \legend{Fonte: Adaptado de \citeonline{Mitchell_Arritmias_MSD}}
\end{figure}

Segundo \citeonline{Mitchell_Arritmias_MSD}, o batimento cardíaco normal se inicia no nódulo sinusal (1), localizado no átrio direito, que atua como o marcapasso natural do coração. A corrente elétrica propaga-se do átrio direito para o esquerdo (2), promovendo sua contração e o bombeamento do sangue para os ventrículos. Em seguida, o impulso atinge o nódulo atrioventricular (3) — conexão entre os átrios e ventrículos — onde é temporariamente retardado, permitindo que os átrios se contraiam completamente e encham as câmaras inferiores.

Posteriormente, a corrente percorre o feixe de His (4), que se divide e conduz o impulso para ambos os ventrículos (5), promovendo sua contração e o bombeamento do sangue para o restante do corpo.

\section{O eletrocardiograma}
\label{sec:ecg}

Segundo \cite{msd_ecg} o eletrocardiograma, ECG, é um exame não invasivo usado para medir a atividade elétrica do coração. Ele é feito a partir do contato de eletrodos, chamados de derivações ou \textit{leads}, sobre a pele.
A quantidade de eletrodos varia, mas geralmente são 12.

Ao registrar a magnitude e direção da corrente, as derivações geram uma onda que representa a atividade elétrica do coração. O passo a passo descrito em \ref{sec:funciionamento_coracao} é refletido em sua morfologia.

Na Figura~\ref{fig:ecg_exemplo_coracao}, é ilustrado um  ECG de um batimento, observe que ele é subdividido em: onda P, complexo QRS e onda T \cite{msd_ecg}. Note que cada uma dessas partes se refere a um estágio do batimento.

\begin{figure}[H]
  \centering
  \caption{Exemplo de ECG com sua morfologia destacada}
   \includegraphics[width=0.7\textwidth]{figuras/ecg_exemplo_coracao.png} % insere o tikzpicture puro
  \label{fig:ecg_exemplo_coracao}
    \legend{Fonte: Adaptado de  \citeonline{msd_ecg}}
\end{figure}

\section{Arritmias}

As doenças cardíacas podem ser diversas. Dentre elas, as arritmias são um grupo que podem ser diagnósticas via ECG.

\subsection{Arritmias clínicas}

Segundo \citeonline{Mitchell_Arritmias_MSD} as arritmias são alterações no ritmo cardíaco que podem ter diversas causas, incluindo alterações hormonais, uso de medicamentos, toxinas (como álcool ou cafeína), anomalias eletrolíticas ou doenças cardíacas.
Em adultos em repouso, a frequência cardíaca normal varia entre 60 e 100 batimentos por minuto (bpm). Frequências mais baixas, conhecidas como bradicardia sinusal, são comuns em atletas, crianças pequenas, adolescentes, jovens adultos e durante o sono. Por outro lado, a taquicardia sinusal ocorre quando a frequência se eleva, podendo ser observada durante o esforço físico, doenças, estimulação neural simpática ou emoção intensa.

O autor observa que variações no ritmo cardíaco são fenômenos fisiológicos normais. Durante a respiração, por exemplo, é comum que a frequência aumente e diminua levemente, comportamento conhecido como arritmia sinusal respiratória. Um ritmo cardíaco perfeitamente regular pode indicar patologias no sistema nervoso autônomo, como ocorre em casos de diabetes avançado. Dessa forma, ainda não existe um indicador global e definitivo do que seria um ritmo sinusal considerado saudável.

As arritmias podem ser classificadas de forma simplificada em três tipos principais:

\begin{enumerate}
    \item \textbf{Taquicardia} — frequência excessivamente rápida;
    \item \textbf{Bradicardia} — frequência excessivamente lenta;
    \item \textbf{Irregular} — quando os impulsos percorrem o coração por vias irregulares.
\end{enumerate}

Observe na Figura~\ref{fig:ecg_exemplo_coracao}, exemplos desses três tipos arrítmicos ilustrados em um ECG.

\subsection{Padrões de classificação para algoritmos (AAMI)}

\label{sub_sec:padroes_arritmias_aami}

A \textit{Association for the Advancement of Medical Instrumentation}, AAMI, define cinco classes de arritmia: normal (N), ventricular (V), supraventricular (S), fusão (F) e não classificado (Q) \cite{silva2025,saadatnejad2020},
que são normalmente usadas para classificação.

\subsubsection{Arritmias Ventriculares}

As arritmias ventriculares são classificadas conforme sua origem e manifestação clínica, que variam de batimentos compensatórios a condições de alto risco. A seguir, são apresentados alguns tipos de arritmia ventricular.

\begin{itemize}
    \item \textbf{Batimento Ventricular de Escape:} Atua como um mecanismo compensatório, funcionando como um "backup" protetivo do coração quando o marcapasso natural falha temporariamente.
    
    \item \textbf{Contração Prematura Ventricular (PVC):} De acordo com \citeonline{Sattar_Premature_2025}, as PVCs são batimentos originários dos ventrículos que podem ocorrer mesmo em indivíduos saudáveis. Sua morfologia é variável, dependendo da origem do impulso, de doenças estruturais ou ainda de uso de medicamentos. Quando frequentes, podem causar fadiga e palpitações, evoluindo para disfunções ventriculares e, em alguns casos, representando a primeira manifestação de cardiopatias estruturais. \citeonline{msdmanuals_ventricular} cita ainda outras causas potenciais de PVCs, como doenças da artéria coronária (especialmente durante ou após infarto), dilatação ventricular decorrente de insuficiência cardíaca e alterações nas válvulas cardíacas.
    
    \item \textbf{Taquicardia Ventricular (TV):} Em \citeonline{mitchell2024vt}, TV é descrito como uma arritmia originada nos ventrículos, produzindo uma frequência cardíaca de até $120$ bpm. É formada por uma sequência de contrações ventriculares prematuras (PVCs). Quando persiste por mais de 30 segundos, recebe a denominação de taquicardia sustentada. Costuma ocorrer em indivíduos com alterações estruturais cardíacas, como infarto do miocárdio, falha cardíaca ou cardiomiopatia. Os sintomas incluem fraqueza, tontura e desconforto torácico. Caso persista por mais de 30 segundos, o tratamento é indicado mesmo na ausência de sintomas, uma vez que pode evoluir para fibrilação ventricular.
    
    \item \textbf{Flutter Ventricular:} Segundo o \citeonline{mesh_ventricular_flutter}, o \textit{flutter} ventricular é caracterizado por uma taquicardia extremamente rápida e instável hemodinamicamente ($150$ a $300$ bpm). É potencialmente fatal e, tipicamente, evolui para a fibrilação ventricular.

    \item \textbf{Fibrilação Ventricular (FV):} Em \citeonline{mitchell2024afib}, a fibrilação ventricular é caracterizada por batimentos rápidos e desordenados, resultantes de sinais elétricos caóticos nos ventrículos. Essa condição leva à perda de consciência em poucos segundos e à morte caso não haja intervenção imediata, configurando-se como um tipo de parada cardíaca. Entre suas causas estão afogamentos, choques elétricos e falha cardíaca.
\end{itemize}

Conforme \citeonline{Sattar_Premature_2025} apesar de ocorrerem também em indivíduos saudáveis, as PVCs possuem significância clínica, pois podem estar associadas a condições mais graves. É importante considerar o contexto do batimento: no caso da taquicardia ventricular, por exemplo, o diagnóstico é estabelecido a partir de uma sequência de PVCs que produz uma frequência cardíaca. Assim, o diagnóstico depende de uma combinação de características temporais e morfológicas, observando-se o contexto dos batimentos e, naturalmente, os sintomas clínicos.

\subsubsection{Arritmias Supraventriculares}

Segundo o \citeonline{texasheart_arrhythmias} as arritmias supraventriculares (S) se originam acima dos ventrículos, como nos átrios ou nos caminhos de condução. Geralmente, são consideradas mais benignas que as arritmias ventriculares e podem ocorrer, assim como os PVCs, em resposta ao consumo de cafeína, tabaco, álcool, tosse ou remédio para resfriado. Outras causas incluem problemas na tireoide. Elas podem causar palpitações, falta de ar ou aperto no peito.

Dentre alguns exemplos de arritmia supraventricular, incluem:

\begin{itemize}
    \item \textbf{Contrações Supraventriculares Prematuras (CSPs):} Ocorrem quando o átrio se contrai de forma antecipada \cite{texasheart_arrhythmias}.
  
    \item \textbf{Taquicardias Supraventriculares (TSV):} Segundo \citeonline{statpearls_svt}, TSVs são formadas por desordens rítmicas rápidas e caracterizadas por um complexo QRS mais estreito (menor que $120$ ms) e uma frequência cardíaca alta. Em adultos, essa frequência é superior a $100$ bpm, enquanto que em crianças, varia de $180$ a $220$ bpm. Os autores apontam como as TSV mais comuns a taquicardia atrioventricular nodal reentrante e a taquicardia atrioventricular recíproca.
    
    \item \textbf{Fibrilação Atrial (FA):} \cite{texasheart_arrhythmias} caracteriza FA como batimentos rápidos e irregulares, resultantes de contrações desordenadas das fibras musculares. É uma das principais causas de Acidente Vascular Cerebral (AVC) em idosos, pois o ritmo desordenado causa o acúmulo de sangue nos átrios, o que pode levar à formação de coágulos que viajam até o cérebro.
\end{itemize}
\subsubsection{Batimentos normais}

Dentro da classe dos batimentos normais, além do batimento típico descrito na \ref{sec:funciionamento_coracao}, incluem-se também os batimentos atriais de escape e os bloqueios do ramo esquerdo e direito. Estes últimos, embora inofensivos por si só, podem indicar condições cardíacas subjacentes mais graves, como doença da artéria coronária ou infarto do miocárdio prévio \cite{mitchell2024hisbloqueio}.

\subsubsection{Conclusão da Fundamentação sobre arritmia e definição do objetivo}

As arritmias cardíacas são classificadas, primariamente, de acordo com sua origem: ventriculares (quando iniciadas nos ventrículos) ou supraventriculares (originadas acima deles, como nos átrios). Tais alterações se manifestam no eletrocardiograma (ECG) como um ritmo cardíaco acelerado, retardado ou com condução elétrica anormal, refletindo-se na morfologia do traçado.

Apesar dessa classificação, a detecção automática enfrenta desafios cruciais devido à alta variabilidade do ECG:
\begin{itemize}
    \item Variações Fisiológicas: Não há padrões universais de atividade cardíaca normal, pois o ECG varia conforme a faixa etária, o nível de atividade física ou as condições clínicas individuais.
    \item Diversidade Intra-classe: A amplitude de manifestações é alta, como o contraste entre as contrações ventriculares prematuras (PVCs) e as taquicardias ventriculares (TVs).
\end{itemize}

Dadas a maior significância clínica e o potencial risco de morte súbita associado à progressão de arritmias ventriculares, o presente trabalho foca na classificação binária da arritmia ventricular (contração ventricular prematura e batimento ventricular de escape), em contraste com a classe de batimentos normais.
Apesar de ocorrerem em indivíduos saudáveis, PVCs estão associados a tipos arrítmicos mais graves, como a taquicardia ventricular e fibrilação atrial, podendo representar um risco maior à vida.

\section{Redes Neurais Artificiais}
\label{sec:ann}

Devido a complexidade dos dados — em partes devido a variabilidade observada tanto intra classe quanto entre pacientes — e também ao volume que 
costuma ser gerado, ECGs são um bom material para a aplicação de algoritmos de aprendizado de máquina, como redes neurais que serão discutidas 
na seguinte seção.

Segundo \citeonline{geron2022hands}, as redes neurais artificiais (ANN) são modelos de inteligência artificial inspirado nos neurônios biológicos
— células encontradas no cérebro de animais responsáveis pela função cognitiva. Elas funcionam como pequenas unidades computacionais que 
conectam-se para executar funções complexas.

\citeonline{james2023} descreve uma ANN como uma função que recebe um conjunto de \textit{p} preditores e, por meio de transformações não lineares, busca prever uma variável resposta Y.
Na Figura \ref{fig:ann_ff_exemplo}, é ilustrada uma arquitetura simples de rede neural \textit{feedforward}.

\begin{figure}[H]
  \centering
  \caption{Exemplo de uma ANN \textit{feed-foward}}
   \includegraphics[width=0.55\textwidth]{figuras/ann_exemplos/ann_ff_exemplo.png} % insere o tikzpicture puro
  \label{fig:ann_ff_exemplo}
    \legend{Fonte: Adaptado de  \citeonline{james2023}}
\end{figure}

Essa rede é composta por três camadas principais: a camada de entrada (\textit{Input layer}), a camada oculta (\textit{Hidden layer}) e a camada de saída (\textit{Output layer}).
Cada neurônio na camada oculta é conectado a todos os neurônios da camada de entrada, assim como os neurônios da camada de saída são conectados aos da camada oculta.

A cada conexão é atribuído um peso, que representa a força da relação entre os neurônios — em analogia à intensidade das sinapses no cérebro biológico \cite{geron2022hands}. Além disso, cada neurônio possui um viés (\textit{bias}), que desloca o ponto de ativação.

A ativação de um neurônio é determinada por uma função de ativação não linear, essencial para permitir que a rede aprenda relações complexas entre as variáveis de entrada.
Sem essa função, uma ANN de múltiplas camadas seria equivalente a uma simples regressão linear. Dentre os exemplos de função de ativação, é possível
citar a sigmoide (ou logística), ReLU (\textit{REctified Linear Unit}) e a tahn, ilustradas na Figura~\ref{fig:funcoes_ativacao_exemplo}.

\begin{figure}[H]
  \centering
  \caption{Funções de ativação Sigmoid, Tanh e ReLU}
  \includegraphics[width=0.65\textwidth]{figuras/ann_exemplos/funcoes_de_ativacao.png}
  \label{fig:funcoes_ativacao_exemplo}
  \legend{Fonte: Elaborado pelo autor.}
\end{figure}

\subsection{Treinamento de redes neurais}

Segundo \citeonline{geron2022hands}, ANNs aprendem através da minimização de uma função de custo. Uma função muito utilizada para problemas de classificação binária
é a \textit{binary cross-entropy} que penaliza quando a probabilidade prevista é distante da classe real. Já a função de ativação 
na camada de saída é a sigmoide.

Para minimizar a função de custo, as redes neurais utilizam um algoritmo, ou variações, chamado de gradiente descendente que ajusta 
os parâmetros (os pesos e vieses) em pequenos passos. O quão grande é esse passo é determinado por um hiperparâmetro denominado taxa 
de aprendizagem, \textit{learning-rate}. O cálculo da contribuição do erro pelos pesos e vieses é feito por um outro algoritmo, 
denominado \textit{backpropagation}.

Conforme \citeonline{geron2022hands}, as redes neurais, especialmente as profundas, podem ser afetas por dois problemas; o gradiente desvanecente (\textit{vanishing gradient})
e o gradiente instável (\textit{exploding gradient}), que afetam a capacidade de aprendizado da rede. No primeiro, 
os gradientes ficam progressivamente menores, diminuindo a atualização dos pesos. Com o tempo, a rede para de aprender. Já no segundo,
os gradientes ficam progressivamente maiores, podendo causar problemas de convergência.

Com o tempo, variações de neurônios foram desenvolvidos, visando explorar determinadas propriedades dos dados. Na próxima seção, 
será apresentado duas variações de neurônios artificiais; a recorrente e a convolucional. A primeira, foi pensada para problemas 
sequenciais e a segunda, tem grande aplicação na área de processamento de imagens e padrões.

\section{Redes Neurais Recorrentes e suas variações}
\label{sec:fundamentos_rnn}

Uma rede neural recorrente — \textit{Recurrent Neural Network} (RNN) — pode ser entendida, de forma intuitiva, como uma rede que possui memória. 
Segundo \citeonline{james2023}, as RNNs se destacam em tarefas que envolvem dados com natureza sequencial, como:

\begin{itemize}
    \item \textbf{Documentos textuais}: como resenhas de livros, filmes, artigos jornalísticos ou \textit{tweets}. 
    Nesses casos, a sequência e a posição relativa das palavras ajudam a capturar a narrativa, os temas e o tom do texto. 
    Entre as aplicações estão a análise de sentimento, a tradução automática e a sumarização de textos.
    
    \item \textbf{Séries temporais}: como temperatura, precipitação, velocidade do vento, qualidade do ar, entre outros. 
    As RNNs podem ser usadas para prever o comportamento dessas variáveis em diferentes horizontes de tempo, de dias a décadas.
    
    \item \textbf{Sinais sonoros}: como voz gravada e música. 
    Aplicações incluem legendagem automática, tradução de fala e classificação de sons.
\end{itemize}

Segundo \citeonline{james2023}, a entrada de uma RNN é uma sequência. 
Por exemplo, em uma tarefa de classificação de documentos, um texto pode ser representado como uma sequência 
\( X = \{X_1, X_2, ..., X_L\} \) de \(L\) elementos, onde cada \(X_l\) representa uma palavra (ou vetor de características da palavra).  

Na Figura \ref{fig:rnn_exemplo}, é ilustrada uma RNN simples com apenas um neurônio em cada camada — de entrada, oculta e saída.

\begin{figure}[H]
  \centering
  \caption{Exemplo de uma RNN}
   \includegraphics[width=0.6\textwidth]{figuras/ann_exemplos/rnn_exemplo.png}
  \label{fig:rnn_exemplo}
  \legend{Fonte: Adaptado de \citeonline{james2023}}
\end{figure}

O \textit{loop} à esquerda representa a retroalimentação da rede.  
À direita, a RNN é ``desenrolada'' no tempo: a rede processa um elemento \(X_l\) de cada vez e produz a ativação \(A_l\), 
calculada a partir tanto da entrada atual quanto da ativação anterior \(A_{l-1}\). 
Essa ativação é então usada para gerar a saída \(O_l\). 
Neste exemplo, apenas a última ativação da sequência é utilizada como saída final.

Mais formalmente, suponha que cada entrada \(X_l\) possua \(p\) componentes:  
\( X_{l}^{T} = (X_{l1}, X_{l2}, ..., X_{lp}) \), 
e que a camada oculta tenha \(K\) unidades:  
\( A_{l}^{T} = (A_{l1}, A_{l2}, ..., A_{lK}) \). 
A matriz \(\mathbf{W}\) representa os \(K \times (p + 1)\) pesos da camada de entrada, 
\(\mathbf{U}\) representa os \(K \times K\) pesos recorrentes (entre ativações sucessivas), 
e \(\mathbf{B}\) é o vetor de pesos da camada de saída. 
A ativação é calculada por:

\begin{equation}
  A_{lk} = g\left( w_{k0} + \sum_{j=1}^{p} w_{kj} X_{lj} + \sum_{s=1}^{K} u_{ks} A_{l-1,s} \right)
\end{equation}

A saída é dada por:

\begin{equation}
  O_{l} = \beta_{0} + \sum_{k=1}^{K} \beta_k A_{lk}
\end{equation}

Em problemas de classificação binária, a função sigmoide pode ser usada na camada de saída.  
Note que as matrizes \(\mathbf{U}\), \(\mathbf{W}\) e \(\mathbf{B}\) são compartilhadas entre todas as etapas temporais, 
o que permite que a rede capture dependências sequenciais nos dados.

Segundo \citeonline{geron2022hands}, a entrada e saída de uma RNN pode ser configurada de três maneiras:

\begin{itemize}
    \item \textbf{Sequencia para sequencia (sequence-to-sequence)}: A entrada e a saída são sequencias. Um exemplo seria a previsão 
    do consumo de energia onde a rede recebe o consumo nos últimos \textit{N} dias e prever os próximos \textit{N - 1} dias.
    
    \item \textbf{Sequencia para vetor (sequence-to-vector)}: Apenas a entrada é uma sequência, como na Figura~\ref{fig:rnn_exemplo}.
    Uma aplicação é análise de sentimentos de uma resenha, onde a rede recebe uma sequência de palavras e deve classificar em:
    gostei ou não gostei.
    
    \item \textbf{Vetor para sequência (vector-to-sequence)}: Apenas a saída é uma sequência. A rede recebe o mesmo vetor 
    em cada passo de tempo. Um exemplo é gerar a legenda de uma imagem. Assim, a rede receberia a imagem ou mesmo a saída
    de uma CNN e baseado nisso, ela gera a legenda.
\end{itemize}

Existem duas variações de RNN muito utilizadas: a LSTM — \textit{long short term memory} e a GRU — \textit{gated recurrent unit}.
Ambas foram desenvolvidas para amenizar o problema do \textit{vanishing gradient} que ocorre na camada recorrente da RNN, o que 
faz com que a rede "esqueça" sequencias longas.

\subsection{Long Short-Term Memory (LSTM)}

\citeonline{geron2022hands} descreve a LSTM (\textit{Long Short-Term Memory}) como uma célula recorrente que combina três componentes principais: 
uma memória de curto prazo, uma memória de longo prazo e um mecanismo de esquecimento. 
Na Figura \ref{fig:lstm_exemplo}, é apresentado o diagrama de uma célula LSTM típica.

\begin{figure}[H]
  \centering
  \caption{Esquema de uma célula LSTM}
  \includegraphics[width=0.55\textwidth]{figuras/ann_exemplos/lstm_celula.png}
  \label{fig:lstm_exemplo}
  \legend{Fonte: Adaptado de \citeonline{geron2022hands}}
\end{figure}

A célula LSTM é composta por quatro unidades \textit{fully connected} (FC), formando uma estrutura semelhante à apresentada na Seção~\ref{sec:ann}.  
As saídas dessas unidades são representadas por \( f_t \), \( i_t \), \( o_t \) e \( g_t \), 
onde as três primeiras correspondem às portas de controle — \textit{forget} (\textbf{f}), \textit{input} (\textbf{i}) e \textit{output} (\textbf{o}) — 
enquanto \( g_t \) é a saída da unidade principal (\textbf{g}).  
As variáveis \( h_t \) e \( c_t \) representam, respectivamente, a memória de curto prazo (\textit{hidden state}) e a memória de longo prazo (\textit{cell state}).

De forma simplificada, o funcionamento de uma LSTM ocorre da seguinte maneira:  
a entrada atual \( X_t \) e a memória de curto prazo anterior \( h_{t-1} \) são fornecidas à célula.  
Essas duas informações são processadas pelas quatro camadas FC, que calculam os vetores \( f_t \), \( i_t \), \( o_t \) e \( g_t \).  

O vetor \( f_t \) atua como um filtro de esquecimento, controlando qual fração da memória anterior \( c_{t-1} \) deve ser mantida.  
Em seguida, o vetor \( i_t \) define o quanto da nova informação \( g_t \) será incorporada à memória.  
O novo estado de memória de longo prazo é então atualizado conforme:

\[
c_t = f_t \odot c_{t-1} + i_t \odot g_t
\]

Por fim, a saída da célula é obtida aplicando uma função tangente hiperbólica sobre \( c_t \), modulada pela porta de saída \( o_t \):

\[
h_t = o_t \odot \tanh(c_t)
\]

onde o operador \(\odot\) indica a multiplicação elemento a elemento (\textit{element-wise}).  

O controle exercido por cada porta ocorre porque suas funções de ativação são do tipo logística (sigmoide), cujo contradomínio é o intervalo \([0,1]\).  
Assim, o valor produzido por cada porta funciona como um coeficiente de passagem: se a ativação for próxima de 0, a informação é bloqueada; 
se for próxima de 1, ela é totalmente transmitida.  
Essa estrutura permite que a LSTM controle de forma adaptativa o fluxo de informação ao longo do tempo, 
mitigando o problema do \textit{vanishing gradient} presente em RNNs tradicionais.

\subsection{Gated Recurrent Unit (GRU)}

Segundo \citeonline{geron2022hands}, a Gated Recurrent Unit (GRU) — proposta por Kyunghyun Cho et al. (2014) — 
é uma versão simplificada da LSTM que mantém desempenho equivalente em diversas tarefas. 
Na Figura~\ref{fig:gru_exemplo}, é apresentado um esquema ilustrativo dessa célula.

\begin{figure}[H]
  \centering
  \caption{Esquema de uma célula GRU}
  \includegraphics[width=0.55\textwidth]{figuras/ann_exemplos/gru_celula.png}
  \label{fig:gru_exemplo}
  \legend{Fonte: Adaptado de \citeonline{geron2022hands}}
\end{figure}

Os vetores de estado \(r_t\), \(z_t\) e \(g_t\) correspondem às três unidades totalmente conectadas 
(\textbf{r}, \textbf{z} e \textbf{g}), enquanto \(h_t\) representa a memória da célula. 
A unidade \textbf{r} controla a quantidade de informação de \(h_{t-1}\) que é transmitida para \textbf{g}, 
enquanto \textbf{z} desempenha o papel do mecanismo de esquecimento. 
Note que há multiplicações elemento a elemento entre \(z_t\) e \(h_{t-1}\), e entre \((1 - z_t)\) e \(g_t\); 
desse modo, para que novas informações sejam incorporadas à memória, parte das informações antigas precisa ser descartada.

O mecanismo de controle da GRU é, portanto, mais simples que o da LSTM, o que se traduz em um número menor 
de parâmetros a serem aprendidos durante o treinamento.

Cabe ressaltar, entretanto, que a simplicidade da GRU não implica necessariamente em melhor desempenho. 
Estudos como \citeonline{narotamo2024} e \citeonline{saadatnejad2020} relatam resultados distintos 
em diferentes contextos e conjuntos de dados.


\section{Redes Neurais Convolucionais}
\label{sec:fundamentos_cnn}

Segundo \citeonline{james2023}, as redes neurais convolucionais — ou CNNs (\textit{Convolutional Neural Networks}) — 
tiveram grande destaque a partir de 2010, especialmente em tarefas de classificação de imagens. 
Esse avanço se deve, em parte, ao surgimento de grandes bases de dados e ao aumento da capacidade computacional, 
mas também à criação de uma nova arquitetura de rede neural, inspirada no funcionamento da percepção visual humana.

As CNNs constroem uma hierarquia de \textit{features} que se tornam progressivamente mais complexas. 
Na Figura~\ref{fig:cnn_tigre}, é apresentado um esquema ilustrativo de como uma CNN reconheceria um tigre: 
nas primeiras camadas são extraídos padrões simples, como bordas e formas básicas; então, elas são combinadas para 
formar os olhos, orelhas, até que, enfim, a rede reconhece o tigre.

\begin{figure}[H]
  \centering
  \caption{Esquema de como uma CNN identificaria um tigre}
  \includegraphics[width=0.55\textwidth]{figuras/ann_exemplos/cnn_tigre.png}
  \label{fig:cnn_tigre}
  \legend{Fonte: Adaptado de \citeonline{james2023}}
\end{figure}

De modo geral, uma CNN é composta por duas estruturas principais: 
as \textit{camadas convolucionais}, responsáveis por extrair características relevantes dos dados, 
e as \textit{camadas de pooling}, que reduzem a dimensionalidade e mantêm apenas as informações mais significativas.

Para compreender o funcionamento de uma convolução, \citeonline{james2023} da o seguinte exemplo.

Considere a seguinte matriz, representando uma imagem:

\[
\begin{bmatrix}
a & b & c \\
d & e & f \\
g & h & i \\
j & k & l \\
\end{bmatrix}
\]

e um filtro convolucional \(2 \times 2\):

\[
\begin{bmatrix}
\alpha & \beta \\
\gamma & \delta
\end{bmatrix}
\]

A operação de convolução consiste em multiplicar o filtro elemento a elemento com cada submatriz \(2 \times 2\) da imagem, 
somar os resultados e, em seguida, deslizar o filtro sobre a matriz. 
O resultado dessa operação é:

\[
\begin{bmatrix}
a\alpha + b\beta + d\gamma + e\delta & 
b\alpha + c\beta + e\gamma + f\delta \\[4pt]
d\alpha + e\beta + g\gamma + h\delta & 
e\alpha + f\beta + h\gamma + i\delta \\[4pt]
g\alpha + h\beta + j\gamma + k\delta & 
h\alpha + i\beta + k\gamma + l\delta \\
\end{bmatrix}
\]

O filtro atua como um detector de padrões locais: se uma submatriz se assemelha ao filtro, ela será destacada na saída. 
É importante notar que a convolução \textbf{não} é uma multiplicação matricial tradicional.

Após a convolução, aplica-se normalmente uma camada de \textit{pooling}, que resume as \textit{features} extraídas.
O \textit{max pooling}, por exemplo, seleciona o maior valor em cada submatriz \(2 \times 2\) não sobreposta:

\[
\text{Max pool} \quad
\begin{bmatrix}
1 & 2 & 5 & 3 \\
3 & 0 & 1 & 2 \\
2 & 1 & 3 & 4 \\
1 & 1 & 2 & 0
\end{bmatrix}
\rightarrow
\begin{bmatrix}
3 & 5 \\
2 & 4
\end{bmatrix}
\]

Na Figura~\ref{fig:cnn_ar_exemplo}, é ilustrada uma arquitetura típica de CNN.

\begin{figure}[H]
  \centering
  \caption{Arquitetura típica de uma CNN}
  \includegraphics[width=1.0\textwidth]{figuras/ann_exemplos/cnn_arquitetura.png}
  \label{fig:cnn_ar_exemplo}
  \legend{Fonte: Adaptado de \citeonline{james2023}}
\end{figure}

A entrada consiste em uma imagem tridimensional, na qual a terceira dimensão — denominada \textit{canal} — 
corresponde às cores no modelo RGB. 
A primeira camada convolucional transforma a imagem inicial em uma nova representação, 
por exemplo, de dimensão \((32, 32, 6)\), onde o número 6 corresponde à quantidade de filtros aplicados. 
Em seguida, uma camada de \textit{pooling} reduz essa dimensão para \((16, 16, 6)\). 
Novas camadas convolucionais e de \textit{pooling} podem ser adicionadas até que, ao final, 
os dados sejam achatados (\textit{flattened}) e passados para camadas totalmente conectadas (\textit{FC}) para a classificação.

De modo geral, cada camada convolucional aumenta a profundidade do canal (número de filtros), 
enquanto as camadas de \textit{pooling} reduzem as dimensões espaciais da imagem.

Por fim, conforme \citeonline{geron2022hands}, CNNs não se limitam a imagens: 
elas também podem ser aplicadas a sinais unidimensionais. 
Nesses casos, um filtro unidimensional é deslizado sobre o sinal, 
permitindo que a rede aprenda padrões locais (com tamanho máximo igual ao do filtro). 
Com múltiplos filtros, obtém-se uma sequência de \textit{features} que podem, em seguida, 
ser processadas por uma RNN, capturando dependências temporais mais longas.

\input{inputs/revisao_literatura}