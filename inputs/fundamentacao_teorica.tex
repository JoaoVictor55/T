\chapter{Fundamentação Teórica}
\label{ch:fundamentacao}

Como o trabalho envolve o reconhecimento de arritmias cardíacas, o primeiro ponto é compreender
como o coração funciona, mais especialmente, como o funciona o mecanismo do impulso elétrico responsáveis pela 
contração do mesmo. Em seguida, entender o princípio de funcionamento das redes neurais, que integra a solução 
para o problema. Por fim, será apresentado alguns trabalhos que correlacionaram as duas áreas.

\section{Funcionamento do coração}
\label{sec:funciionamento_coracao}

O coração é um órgão muscular composto por quatro câmaras — átrio direito e esquerdo, e ventrículo direito e esquerdo — que se contraem de forma rítmica, bombeando sangue para o corpo. Essas contrações são controladas por correntes elétricas que percorrem o coração de maneira precisa e em velocidade controlada.

Na Figura~\ref{fig:coracao_esquema_eletrico}, o sistema de condução elétrica do coração é ilustrado.

\begin{figure}[H]
  \centering
  \caption{Sistema de condução do coração}
   \includegraphics[width=0.35\textwidth]{figuras/coracao_sistema_eletrico.png} % insere o tikzpicture puro
  \label{fig:coracao_esquema_eletrico}
    \legend{Fonte: Adaptado de \citeonline{Mitchell_Arritmias_MSD}}
\end{figure}

Segundo \citeonline{Mitchell_Arritmias_MSD}, o batimento cardíaco normal se inicia no nódulo sinusal (1), localizado no átrio direito, que atua como o marcapasso natural do coração. A corrente elétrica propaga-se do átrio direito para o esquerdo (2), promovendo sua contração e o bombeamento do sangue para os ventrículos. Em seguida, o impulso atinge o nódulo atrioventricular (3) — conexão entre os átrios e ventrículos — onde é temporariamente retardado, permitindo que os átrios se contraiam completamente e encham as câmaras inferiores.

Posteriormente, a corrente percorre o feixe de His (4), que se divide e conduz o impulso para ambos os ventrículos (5), promovendo sua contração e o bombeamento do sangue para o restante do corpo.

\section{O eletrocardiograma}
\label{sec:ecg}

O eletrocardiograma, ECG, é um exame não invasivo usado para medir a atividade elétrica do coração. Ele é feito a partir do contato de eletrodos, chamados de derivações ou \textit{leads}, sobre a pele.
A quantidade de eletrodos varia, mas geralmente são 12 \cite{msd_ecg}.

Ao registrar a magnitude e direção da corrente, as derivações geram uma onda que representa a atividade elétrica do coração. O passo a passo descrito em \ref{sec:funciionamento_coracao} é refletido em sua morfologia.

Na Figura~\ref{fig:ecg_exemplo_coracao}, é ilustrado um  ECG de um batimento, observe que ele é subdividido em: onda P, complexo QRS e onda T \cite{msd_ecg}. Note que cada uma dessas partes se refere a um estágio do batimento.

\begin{figure}[H]
  \centering
  \caption{Exemplo de ECG com sua morfologia destacada}
   \includegraphics[width=0.7\textwidth]{figuras/ecg_exemplo_coracao.png} % insere o tikzpicture puro
  \label{fig:ecg_exemplo_coracao}
    \legend{Fonte: Adaptado de  \citeonline{msd_ecg}}
\end{figure}

\section{Arritmias}

As doenças cardíacas podem ser diversas. Dentre elas, as arritmias são um grupo que podem ser diagnósticas via ECG

\subsection{Arritmias clínicas}

As arritmias são alterações no ritmo cardíaco que podem ter diversas causas, incluindo alterações hormonais, uso de medicamentos, toxinas (como álcool ou cafeína), anomalias eletrolíticas ou doenças cardíacas.
Segundo \citeonline{Mitchell_Arritmias_MSD}, em adultos em repouso, a frequência cardíaca normal varia entre 60 e 100 batimentos por minuto (bpm). Frequências mais baixas, conhecidas como bradicardia sinusal, são comuns em atletas, crianças pequenas, adolescentes, jovens adultos e durante o sono. Por outro lado, a taquicardia sinusal ocorre quando a frequência se eleva, podendo ser observada durante o esforço físico, doenças, estimulação neural simpática ou emoção intensa.

Variações no ritmo cardíaco são fenômenos fisiológicos normais. Durante a respiração, por exemplo, é comum que a frequência aumente e diminua levemente, comportamento conhecido como arritmia sinusal respiratória. O autor ainda observa que um ritmo cardíaco perfeitamente regular pode indicar patologias no sistema nervoso autônomo, como ocorre em casos de diabetes avançado. Dessa forma, ainda não existe um indicador global e definitivo do que seria um ritmo sinusal considerado saudável.


As arritmias podem ser classificadas de forma simplificada em três tipos principais:

\begin{enumerate}
    \item \textbf{Taquicardia} — frequência excessivamente rápida;
    \item \textbf{Bradicardia} — frequência excessivamente lenta;
    \item \textbf{Irregular} — quando os impulsos percorrem o coração por vias irregulares.
\end{enumerate}

Observe na Figura~\ref{fig:ecg_exemplo_coracao}, exemplos desses três tipos arrítmicos ilustrados em um ECG.

\subsection{Padrões de classificação para algoritmos (AAMI)}

\label{sub_sec:padroes_arritmias_aami}

A \textit{Association for the Advancement of Medical Instrumentation}, AAMI, define cinco classes de arritmia: normal (N), ventricular (V), supraventricular (S), fusão (F) e não classificado (Q) \cite{silva2025,saadatnejad2020}.

\subsubsection{Arritmias Ventriculares}

As arritmias ventriculares incluem, por exemplo, as contrações prematuras ventriculares (PVCs) e os batimentos de escape. 
O batimento ventricular de escape atua como um mecanismo compensatório, funcionando como um “backup” protetivo do coração quando o marcapasso natural falha temporariamente.
De acordo com \citeonline{Sattar_Premature_2025}, as PVCs são batimentos originários dos ventrículos que podem ocorrer mesmo em indivíduos saudáveis. Sua morfologia é variável, dependendo da origem do impulso, de doenças estruturais ou ainda de uso de medicamentos. Quando frequentes, podem causar fadiga e palpitações, evoluindo para disfunções ventriculares e, em alguns casos, representando a primeira manifestação de cardiopatias estruturais.

\citeonline{msdmanuals_ventricular} cita ainda outras causas potenciais de PVCs, como doenças da artéria coronária (especialmente durante ou após infarto), dilatação ventricular decorrente de insuficiência cardíaca e alterações nas válvulas cardíacas. Em pacientes com doenças estruturais, as PVCs podem progredir para arritmias mais graves, como taquicardia ventricular (VT) e fibrilação ventricular (FV), que representam risco de morte súbita.

\citeonline{mitchell2024afib,mitchell2024vt} descrevem tanto a fibrilação quanto a taquicardia ventricular como arritmias originadas nos ventrículos.

A taquicardia ventricular (TV) produz uma frequência cardíaca de até 120 bpm e é formada por uma sequência de contrações ventriculares prematuras (PVCs). Quando persiste por mais de 30 segundos, recebe a denominação de taquicardia sustentada. Costuma ocorrer em indivíduos com alterações estruturais cardíacas, como infarto do miocárdio, falha cardíaca ou cardiomiopatia. Os sintomas incluem fraqueza, tontura e desconforto torácico. Caso persista por mais de 30 segundos, o tratamento é indicado mesmo na ausência de sintomas, uma vez que pode evoluir para fibrilação ventricular.

Outro tipo de arritmia ventricular é o \textit{flutter} ventricular. Segundo o \citeonline{mesh_ventricular_flutter}, elas são caracterizadas por uma taquicardia extremamente rápida e instável hemodinamicamente (150 a 300 bpm).
São potencialmente fatais e, tipicamente, evoluem para a fibrilação ventricular.

A fibrilação ventricular (FV), por outro lado, é caracterizada por batimentos rápidos e desordenados, resultantes de sinais elétricos caóticos nos ventrículos. Essa condição leva à perda de consciência em poucos segundos e à morte caso não haja intervenção imediata, configurando-se como um tipo de parada cardíaca. Entre suas causas estão afogamentos, choques elétricos e falha cardíaca.

Apesar de ocorrerem também em indivíduos saudáveis, as PVCs possuem significância clínica, pois podem estar associadas a outros tipos de arritmias ventriculares graves. É importante considerar o contexto do batimento: no caso da taquicardia ventricular, por exemplo, o diagnóstico é estabelecido a partir de uma sequência de PVCs que produz uma frequência cardíaca elevada (>20 bpm). Assim, o diagnóstico depende de uma combinação de características temporais e morfológicas, observando-se o contexto dos batimentos e, naturalmente, os sintomas clínicos.

Dentro da classe dos batimentos normais, além do batimento típico descrito na \ref{sec:funciionamento_coracao}, incluem-se também os batimentos atriais de escape e os bloqueios do ramo esquerdo e direito. Estes últimos, embora inofensivos por si só, podem indicar condições cardíacas subjacentes mais graves, como doença da artéria coronária ou infarto do miocárdio prévio \cite{mitchell2024hisbloqueio}.

Segundo o Texas Heart Institute \cite{texasheart_arrhythmias}, arritmias superventricular se originam acima dos ventrículos, como nos 
átrios ou nos caminhos de condução. Geralmente, são mais benignas que as arritmias ventriculares podendo ocorrer, assim como os PVCs, em 
resposta ao consumo de cafeína, tabaco, álcool, tosse ou remédio para resfriado. Outras causas incluem problemas na tireoide. Elas podem causar
palpitações, falta de ar ou aperto no peito.

As contrações superventriculares prematuras, por exemplo, ocorrem quando o átrio se contrai muito cedo. A fibrilação atrial, por outro lado,
são batimentos rápidos e irregulares, causadas por contrações desordenadas das fibras musculares. São as principais causas de 
AVC em idosos, pois causam o acúmulo de sangue nos átrios que podem coagular e viajar até o cérebro. 

\citeonline{statpearls_svt} descreve outros tipos de arritmias superventriculares, como as traquicardias superventricular que são formadas 
por desordens rítmicas rápidas, taquicardia, que são caracterizadas por um complexo QRS mais estreito, menor que 120 ms, e uma 
frequência cardíaca alta. Em adultos, ela é superior a 100 bpm enquanto que em crianças, varia de 180 à 220 bpm. 

Os autores apontam como as traquicardias superventriculares mais comuns a taquicardia atrioventricular nodal reentrante e a taquicardia
atrioventricular recíproca. 

Portanto, as arritmias podem ser classificadas de acordo com sua origem — ventriculares, quando se iniciam nos ventrículos, ou supraventriculares, quando se originam acima deles, como nos átrios.
Essas alterações podem se manifestar por um ritmo cardíaco acelerado ou retardado, ou ainda por uma condução elétrica anormal, o que se reflete na morfologia do traçado eletrocardiográfico (ECG).

Não há, entretanto, padrões universais que definam o que é uma atividade cardíaca normal, pois o ECG apresenta variações fisiológicas relacionadas, por exemplo, à faixa etária, nível de atividade física ou condições clínicas individuais. Diversos fatores podem influenciar a forma do sinal e gerar anomalias que nem sempre têm significado patológico.

Por fim, a diversidade das arritmias também se expressa dentro de uma mesma classe. No caso das arritmias ventriculares, por exemplo, é possível observar desde contrações ventriculares prematuras (PVCs) até taquicardias ventriculares (TVs), ilustrando a amplitude de manifestações possíveis.

\section{Redes Neurais Artificiais}

\textcolor{red}{Devido a complexidade dos dados — em partes devido a variabilidade observada tanto intra classe quanto entre pacientes — e também ao volume que 
costuma ser gerado, ECGs são um bom material para a aplicação de algoritmos de aprendizado de máquina, como redes neurais artificiais.
Portanto, na próxima seção será apresentado o conceito de redes neurais artificias}

\label{sec:ann}

Segundo \citeonline{geron2022hands}, as redes neurais artificiais (ANN) foram introduzidas em 1943 pelo neurofisiologista Warren McCulloch e pelo matemático Walter Pitts como um modelo simplificado de como os neurônios biológicos podem realizar cálculos complexos por meio de lógica proposicional.

O sucesso inicial levou à crença de que tais modelos poderiam ser usados no desenvolvimento de sistemas verdadeiramente inteligentes. No entanto, em parte devido às limitações na capacidade de processamento e à escassez de dados disponíveis, as ANNs foram gradualmente abandonadas em favor de outras técnicas, como as máquinas de vetor de suporte (\textit{support vector machines} — SVMs).

Nos últimos anos, entretanto, com a criação de grandes conjuntos de dados, o aumento do poder computacional proporcionado pelas GPUs e o desenvolvimento de novas arquiteturas, as ANNs passaram por um reavivamento, consolidando-se como uma das técnicas mais promissoras da inteligência artificial.

O autor descreve os neurônios biológicos como células formadas por um corpo celular e uma longa extensão chamada axônio. A partir do corpo celular, partem diversas ramificações denominadas dendritos. Na extremidade do axônio, ocorrem ramificações menores chamadas telodendriais, cujas pontas possuem estruturas denominadas terminais sinápticos.

O axônio de um neurônio conecta-se ao dendrito de outro, formando uma sinapse — o ponto de comunicação entre as células.

Esses neurônios transmitem sinais elétricos chamados potenciais de ação (\textit{action potentials} — APs), que percorrem o axônio até os terminais sinápticos, onde estimulam a liberação de substâncias químicas conhecidas como neurotransmissores. Esses neurotransmissores permitem a comunicação entre os neurônios, podendo estimular ou inibir o disparo de novos potenciais de ação.

Bilhões de neurônios interligam-se por meio de sinapses, formando redes altamente complexas. A ação coordenada dessas redes possibilita a realização de cálculos e processos cognitivos sofisticados, que inspiraram o desenvolvimento das redes neurais artificiais.

Esse é o princípio em que se baseiam as redes neurais artificiais (ANNs): pequenas unidades computacionais simples que, ao se conectarem, são capazes de realizar funções complexas. É importante destacar que um neurônio artificial, no contexto das ANNs, não é uma réplica de um neurônio biológico, mas sim uma abstração matemática inspirada em seu funcionamento.

Segundo \citeonline{james2023}, uma ANN pode ser vista como uma função que recebe um conjunto de  p preditores e, por meio de transformações não lineares, busca prever uma variável resposta Y.
Na Figura \ref{fig:ann_ff_exemplo}, é ilustrada uma arquitetura simples de rede neural \textit{feedforward}.

\begin{figure}[H]
  \centering
  \caption{Exemplo de uma ANN \textit{feed-foward}}
   \includegraphics[width=0.55\textwidth]{figuras/ann_exemplos/ann_ff_exemplo.png} % insere o tikzpicture puro
  \label{fig:ann_ff_exemplo}
    \legend{Fonte: Adaptado de  \citeonline{james2023}}
\end{figure}

Essa rede é composta por três camadas principais: a camada de entrada (\textit{Input layer}), a camada oculta (\textit{Hidden layer}) e a camada de saída (\textit{Output layer}).
Cada neurônio na camada oculta é conectado a todos os neurônios da camada de entrada, assim como os neurônios da camada de saída são conectados aos da camada oculta.

A cada conexão é atribuído um peso, que representa a força da relação entre os neurônios — em analogia à intensidade das sinapses no cérebro biológico. Além disso, cada neurônio possui um viés (\textit{bias}), que desloca o ponto de ativação.

A ativação de um neurônio é determinada por uma função de ativação não linear, essencial para permitir que a rede aprenda relações complexas entre as variáveis de entrada.
Sem essa função, uma ANN de múltiplas camadas seria equivalente a uma simples regressão linear.

Formalmente, uma ANN pode ser descrita como uma função  f(X) que recebe um vetor de p preditores X=(X1,X2,…,Xp) e produz uma previsão para a variável resposta Y.

No caso da rede ilustrada na Figura \ref{fig:ann_ff_exemplo}, essa função  f pode ser expressa como:

\begin{equation}
f(X) = \beta_0 + \sum_{k=1}^{K} \beta_k h_k(X)
\end{equation}

Onde K refere-se a quantidade de neurônios na camada oculta. \textit{h} é expandido em:

\begin{equation}
h_k = g(w_{k0} + \sum_{j=1}^{p}w_{kj}X_j)
\end{equation}

Assim, \textit{f} é construída em dois passos; primeiro, K ativações da camada oculta são calculados:

\begin{equation}
  A_k = h_k(X) = g(w_{k0} + \sum_{j=1}^{p}w_{kj}X_k)
\end{equation}

Onde \( g(z) \) é a função de ativação não linear. 
Os parâmetros \( \beta_0, \dots, \beta_K \) e \( \omega_0, \dots, \omega_{Kp} \) precisam ser estimados — isto é, aprendidos a partir dos dados.

Existem diversas funções de ativação propostas ao longo do tempo. 
A sigmoide, por exemplo, foi amplamente utilizada nas primeiras redes neurais:

\begin{equation}
  g(z) = \frac{e^z}{1 + e^z} = \frac{1}{1 + e^{-z}}
\end{equation}

Segundo \citeonline{geron2022hands}, essa função é uma aproximação do comportamento de ativação observado em neurônios biológicos. 
Entretanto, atualmente ela é pouco utilizada nas camadas intermediárias, uma vez que é propensa ao problema do \textit{vanishing gradient} e não é centrada em zero, o que prejudica a convergência do modelo.  
Apesar disso, a sigmoide ainda é frequentemente empregada na camada de saída em problemas de classificação binária, pois seu contradomínio é [0, 1]; o que
torna simples interpretar a saída dos neurônios como probabilidades.

Outra função comumente utilizada é a tangente hiperbólica:

\begin{equation}
  g(z) = \tanh(z) = 2\sigma(2z) - 1
\end{equation}

Essa função é centrada em zero e possui uma derivada mais acentuada, o que ajuda a mitigar o problema do \textit{vanishing gradient}.

Atualmente, a função de ativação mais popular é a ReLU (\textit{Rectified Linear Unit}), definida como:

\begin{equation}
g(z) = z_+ =
\begin{cases}
0, & \text{se } z < 0 \\
z, & \text{caso contrário}
\end{cases}
\end{equation}

A ReLU apresenta um cálculo simples e eficiente, além de acelerar o treinamento de redes profundas. 
Embora não seja centrada em zero, o viés dos neurônios auxilia a compensar esse deslocamento.  
Na Figura \ref{fig:funcoes_ativacao_exemplo}, são ilustradas as três funções de ativação discutidas.

\begin{figure}[H]
  \centering
  \caption{Funções de ativação Sigmoid, Tanh e ReLU}
  \includegraphics[width=0.6\textwidth]{figuras/ann_exemplos/funcoes_de_ativacao.png}
  \label{fig:funcoes_ativacao_exemplo}
  \legend{Fonte: Elaborado pelo autor.}
\end{figure}

\subsection{Treinamento de redes neurais}

ANNs aprendem através da minimização de uma função de custo. Uma função muito utilizada para problemas de classificação binária
é a \textit{binary cross-entropy}:

\begin{equation}
-\frac{1}{N}\sum_{i=1}^{N}[y_i\log(p_i) + (1 - y_i)\log(1 - p_i)]
\label{for:cross_entropy}
\end{equation}

Onde \( N \) é a quantidade de amostras.

Como já discutido, na camada de saída costuma-se utilizar a função de ativação sigmoide.
Assim, uma interpretação para a Equação \ref{for:cross_entropy} é que ela penaliza o modelo quando a probabilidade estimada para uma amostra 
se distancia de sua classe real. Por exemplo, considere que uma amostra pertença à classe positiva (\(y = 1\)); se o modelo prevê uma 
probabilidade de apenas 0,01, o erro será significativamente maior do que se tivesse previsto 0,6.

Para minimizar essa função de custo, redes neurais artificiais utilizam algoritmos baseados no \textit{gradiente descendente}. 
Segundo \citeonline{geron2022hands}, o gradiente descendente busca encontrar o mínimo de uma função ajustando iterativamente seus parâmetros. 
Intuitivamente, a função de custo define uma superfície de erro, e o modelo inicia em um ponto aleatório dessa superfície — 
isto é, com pesos inicializados aleatoriamente. 

A cada iteração, o gradiente da função é calculado no ponto atual, indicando a direção de maior crescimento da função de custo. 
Os parâmetros são então atualizados na direção oposta, ou seja, na direção de maior decaimento. 
O tamanho desse passo é controlado por um hiperparâmetro chamado taxa de aprendizado (\textit{learning rate}), denotado por \( \eta \). 
Formalmente, a atualização é dada por:

\begin{equation}
\theta^{(i+1)} = \theta^{(i)} - \eta \nabla f(\theta)
\label{for:gradiente_descendente}
\end{equation}

onde \( \theta \) representa o vetor de parâmetros do modelo (pesos e vieses), e \( \nabla f(\theta) \) é o gradiente da função de custo 
em relação a esses parâmetros.

Na prática, utiliza-se variações do algoritmo, como o \textit{stochastic gradient descent} (SGD), que atualiza os parâmetros a partir de 
amostras individuais ou pequenos subconjuntos (\textit{mini-batches}) dos dados, e otimizadores mais sofisticados como o Adam, que ajusta 
automaticamente a taxa de aprendizado ao longo do treinamento.

O processo de calculo dos gradientes é realizado por outro algoritmo, chamado de \textit{backpropagation}. 
Segundo \citeonline{geron2022hands}, um dos principais obstáculos históricos para o uso eficiente de redes neurais artificiais 
era a dificuldade de ajustar seus parâmetros quando a rede possuía muitas camadas. 
Aprofundar a rede, entretanto, é essencial para que ela aprenda representações (\textit{features}) mais complexas dos dados.

Em 1985, David Rumelhart, Geoffrey Hinton e Ronald Williams demonstraram que um algoritmo proposto anteriormente por Seppo Linnainmaa, em 1970, 
poderia ser aplicado para esse fim. Esse algoritmo, atualmente conhecido como \textit{backpropagation}, tornou possível o treinamento eficiente 
de redes com múltiplas camadas.

De forma simplificada, o \textit{backpropagation} é composto por duas etapas principais: \textit{forward pass} e \textit{backward pass}.

Na fase \textit{forward}, os dados de entrada percorrem a rede camada por camada. 
Cada neurônio realiza seu cálculo, multiplicando os valores de entrada pelos pesos correspondentes, somando o viés e aplicando a função de ativação. 
O resultado dessa sequência de operações é a saída da rede, que é então comparada com o valor real (\textit{ground truth}) 
por meio de uma função de perda, que mede o erro do modelo.

Na fase \textit{backward}, o algoritmo utiliza a regra da cadeia do cálculo diferencial para determinar quanto cada parâmetro 
(peso e viés) contribuiu para o erro total. 
Esse cálculo é feito de trás para frente, da camada de saída até a camada de entrada, ajustando os gradientes de forma eficiente. 
Dessa maneira, cada peso recebe uma medida precisa de sua influência no erro observado.

Por fim, os parâmetros são atualizados aplicando-se o gradiente descendente — ou outra técnica de otimização —, 
movendo os pesos na direção que reduz o erro da rede.

Redes profundas, entretanto, podem ser propensas a um problema denominado genericamente como gradiente instáveis que se manifesta 
de duas formas distintas: desvanecimento do gradiente — \textit{vanishing gradient}  —  e a explosão do gradiente  — \textit{exploding gradient}  — 
que ocorre quando os pesos ficam exponencialmente pequenos ou exponencialmente grandes respectivamente.

No primeiro caso, a atualização torna-se cada vez menor conforme o gradiente flui para as camadas mais iniciais. Na prática
a rede para de aprender. Por exemplo, considere uma rede com três camadas. Se a taxa de contribuição do gradiente 
da camada três for 0,5 e o gradiente local da camada dois for 0,3, a contribuição do erro nesta camada (devido a regra da cadeia)
será de \(0,5 \times 0,3 = 0,15\). Na primeira camada que possui um gradiente local de 0,1, o erro seria de \(0,15 \times 0,1 = 0,015\)
Com mais camadas, esse erro seria multiplicado por gradientes cada vez menores, chegando a zero nas camadas iniciais. 
Como pode ser visto na equação \ref{for:gradiente_descendente}, isso significa que esses pesos não seriam atualizados.

\section{Redes Neurais Recorrentes e suas variações}
\label{sec:fundamentos_rnn}

Uma rede neural recorrente — \textit{Recurrent Neural Network} (RNN) — pode ser entendida, de forma intuitiva, como uma rede que possui memória. 
Segundo \citeonline{james2023}, as RNNs se destacam em tarefas que envolvem dados com natureza sequencial, como:

\begin{itemize}
    \item \textbf{Documentos textuais} — como resenhas de livros, filmes, artigos jornalísticos ou \textit{tweets}. 
    Nesses casos, a sequência e a posição relativa das palavras ajudam a capturar a narrativa, os temas e o tom do texto. 
    Entre as aplicações estão a análise de sentimento, a tradução automática e a sumarização de textos.
    
    \item \textbf{Séries temporais} — como temperatura, precipitação, velocidade do vento, qualidade do ar, entre outros. 
    As RNNs podem ser usadas para prever o comportamento dessas variáveis em diferentes horizontes de tempo, de dias a décadas.
    
    \item \textbf{Sinais sonoros} — como voz gravada e música. 
    Aplicações incluem legendagem automática, tradução de fala e classificação de sons.
\end{itemize}

A entrada de uma RNN é uma sequência. 
Por exemplo, em uma tarefa de classificação de documentos, um texto pode ser representado como uma sequência 
\( X = \{X_1, X_2, ..., X_L\} \) de \(L\) elementos, onde cada \(X_l\) representa uma palavra (ou vetor de características da palavra).  

Na Figura \ref{fig:rnn_exemplo}, é ilustrada uma RNN simples com apenas um neurônio em cada camada — de entrada, oculta e saída.

\begin{figure}[H]
  \centering
  \caption{Exemplo de uma RNN}
   \includegraphics[width=0.6\textwidth]{figuras/ann_exemplos/rnn_exemplo.png}
  \label{fig:rnn_exemplo}
  \legend{Fonte: Adaptado de \citeonline{james2023}}
\end{figure}

O \textit{loop} à esquerda representa a retroalimentação da rede.  
À direita, a RNN é ``desenrolada'' no tempo: a rede processa um elemento \(X_l\) de cada vez e produz a ativação \(A_l\), 
calculada a partir tanto da entrada atual quanto da ativação anterior \(A_{l-1}\). 
Essa ativação é então usada para gerar a saída \(O_l\). 
Neste exemplo, apenas a última ativação da sequência é utilizada como saída final.

Mais formalmente, suponha que cada entrada \(X_l\) possua \(p\) componentes:  
\( X_{l}^{T} = (X_{l1}, X_{l2}, ..., X_{lp}) \), 
e que a camada oculta tenha \(K\) unidades:  
\( A_{l}^{T} = (A_{l1}, A_{l2}, ..., A_{lK}) \). 
A matriz \(\mathbf{W}\) representa os \(K \times (p + 1)\) pesos da camada de entrada, 
\(\mathbf{U}\) representa os \(K \times K\) pesos recorrentes (entre ativações sucessivas), 
e \(\mathbf{B}\) é o vetor de pesos da camada de saída. 
A ativação é calculada por:

\begin{equation}
  A_{lk} = g\left( w_{k0} + \sum_{j=1}^{p} w_{kj} X_{lj} + \sum_{s=1}^{K} u_{ks} A_{l-1,s} \right)
\end{equation}

A saída é dada por:

\begin{equation}
  O_{l} = \beta_{0} + \sum_{k=1}^{K} \beta_k A_{lk}
\end{equation}

Em problemas de classificação binária, a função sigmoide pode ser usada na camada de saída.  
Note que as matrizes \(\mathbf{U}\), \(\mathbf{W}\) e \(\mathbf{B}\) são compartilhadas entre todas as etapas temporais, 
o que permite que a rede capture dependências sequenciais nos dados.

Segundo \citeonline{geron2022hands}, a entrada e saída de uma RNN pode ser configurada de três maneiras:

\begin{itemize}
    \item \textbf{Sequencia para sequencia (sequence-to-sequence)} —  A entrada e a saída são sequencias. Um exemplo seria a previsão 
    do consumo de energia onde a rede recebe o consumo nos últimos \textit{N} dias e prever os próximos \textit{N - 1} dias.
    
    \item \textbf{Sequencia para vetor (sequence-to-vector)} — Apenas a entrada é uma sequência, como na figura \ref{fig:rnn_exemplo}.
    Uma aplicação é análise de sentimentos de uma resenha, onde a rede recebe uma sequência de palavras e deve classificar em:
    gostei ou não gostei.
    
    \item \textbf{Vetor para sequência (vector-to-sequence)} — Apenas a saída é uma sequência. A rede recebe o mesmo vetor 
    em cada passo de tempo. Um exemplo é gerar a legenda de uma imagem. Assim, a rede receberia a imagem ou mesmo a saída
    de uma CNN e baseado nisso, ela gera a legenda.
\end{itemize}

Existem duas variações de RNN muito utilizadas: a LSTM — \textit{long short term memory} e a GRU — \textit{gated recurrent unit}.
Ambas foram desenvolvidas para amenizar o problema do \textit{vanishing gradient} que ocorre na camada recorrente da RNN, o que 
faz com que a rede "esqueça" sequencias longas.

\subsection{Long Short-Term Memory (LSTM)}

\citeonline{geron2022hands} descreve a LSTM (\textit{Long Short-Term Memory}) como uma célula recorrente que combina três componentes principais: 
uma memória de curto prazo, uma memória de longo prazo e um mecanismo de esquecimento. 
Na Figura \ref{fig:lstm_exemplo}, é apresentado o diagrama de uma célula LSTM típica.

\begin{figure}[H]
  \centering
  \caption{Esquema de uma célula LSTM}
  \includegraphics[width=0.55\textwidth]{figuras/ann_exemplos/lstm_celula.png}
  \label{fig:lstm_exemplo}
  \legend{Fonte: Adaptado de \citeonline{geron2022hands}}
\end{figure}

A célula LSTM é composta por quatro unidades \textit{fully connected} (FC), formando uma estrutura semelhante à apresentada na Seção~\ref{sec:ann}.  
As saídas dessas unidades são representadas por \( f_t \), \( i_t \), \( o_t \) e \( g_t \), 
onde as três primeiras correspondem às portas de controle — \textit{forget} (\textbf{f}), \textit{input} (\textbf{i}) e \textit{output} (\textbf{o}) — 
enquanto \( g_t \) é a saída da unidade principal (\textbf{g}).  
As variáveis \( h_t \) e \( c_t \) representam, respectivamente, a memória de curto prazo (\textit{hidden state}) e a memória de longo prazo (\textit{cell state}).

De forma simplificada, o funcionamento de uma LSTM ocorre da seguinte maneira:  
a entrada atual \( X_t \) e a memória de curto prazo anterior \( h_{t-1} \) são fornecidas à célula.  
Essas duas informações são processadas pelas quatro camadas FC, que calculam os vetores \( f_t \), \( i_t \), \( o_t \) e \( g_t \).  

O vetor \( f_t \) atua como um filtro de esquecimento, controlando qual fração da memória anterior \( c_{t-1} \) deve ser mantida.  
Em seguida, o vetor \( i_t \) define o quanto da nova informação \( g_t \) será incorporada à memória.  
O novo estado de memória de longo prazo é então atualizado conforme:

\[
c_t = f_t \odot c_{t-1} + i_t \odot g_t
\]

Por fim, a saída da célula é obtida aplicando uma função tangente hiperbólica sobre \( c_t \) — cuja fórmula é dada em \ref{sec:ann} —, modulada pela porta de saída \( o_t \):

\[
h_t = o_t \odot \tanh(c_t)
\]

onde o operador \(\odot\) indica a multiplicação elemento a elemento (\textit{element-wise}).  

O controle exercido por cada porta ocorre porque suas funções de ativação são do tipo logística (sigmoide), cujo contradomínio é o intervalo \([0,1]\).  
Assim, o valor produzido por cada porta funciona como um coeficiente de passagem: se a ativação for próxima de 0, a informação é bloqueada; 
se for próxima de 1, ela é totalmente transmitida.  
Essa estrutura permite que a LSTM controle de forma adaptativa o fluxo de informação ao longo do tempo, 
mitigando o problema do \textit{vanishing gradient} presente em RNNs tradicionais.

\subsection{GRU}

Segundo \citeonline{geron2022hands}, a Gated Recurrent Unit (GRU) — proposta por Kyunghyun Cho et al. (2014) — 
é uma versão simplificada da LSTM que mantém desempenho equivalente em diversas tarefas. 
Na figura \ref{fig:gru_exemplo}, é apresentado um esquema ilustrativo dessa célula.

\begin{figure}[H]
  \centering
  \caption{Esquema de uma célula GRU}
  \includegraphics[width=0.55\textwidth]{figuras/ann_exemplos/gru_celula.png}
  \label{fig:gru_exemplo}
  \legend{Fonte: Adaptado de \citeonline{geron2022hands}}
\end{figure}

Os vetores de estado \(r_t\), \(z_t\) e \(g_t\) correspondem às três unidades totalmente conectadas 
(\textbf{r}, \textbf{z} e \textbf{g}), enquanto \(h_t\) representa a memória da célula. 
A unidade \textbf{r} controla a quantidade de informação de \(h_{t-1}\) que é transmitida para \textbf{g}, 
enquanto \textbf{z} desempenha o papel do mecanismo de esquecimento. 
Note que há multiplicações elemento a elemento entre \(z_t\) e \(h_{t-1}\), e entre \((1 - z_t)\) e \(g_t\); 
desse modo, para que novas informações sejam incorporadas à memória, parte das informações antigas precisa ser descartada.

O mecanismo de controle da GRU é, portanto, mais simples que o da LSTM, o que se traduz em um número menor 
de parâmetros a serem aprendidos durante o treinamento.

Cabe ressaltar, entretanto, que a simplicidade da GRU não implica necessariamente em melhor desempenho. 
Estudos como \citeonline{narotamo2024} e \citeonline{saadatnejad2020} relatam resultados distintos 
em diferentes contextos e conjuntos de dados.


\section{Redes Neurais Convolucionais}
\label{sec:fundamentos_cnn}

Segundo \citeonline{james2023}, as redes neurais convolucionais — ou CNNs (\textit{Convolutional Neural Networks}) — 
tiveram grande destaque a partir de 2010, especialmente em tarefas de classificação de imagens. 
Esse avanço se deve, em parte, ao surgimento de grandes bases de dados e ao aumento da capacidade computacional, 
mas também à criação de uma nova arquitetura de rede neural, inspirada no funcionamento da percepção visual humana.

As CNNs constroem uma hierarquia de \textit{features} que se tornam progressivamente mais complexas. 
Na figura \ref{fig:cnn_tigre}, é apresentado um esquema ilustrativo de como uma CNN reconheceria um tigre: 
nas primeiras camadas são extraídos padrões simples, como bordas e formas básicas; então, elas são combinadas para 
formar os olhos, orelhas, até que, enfim, a rede reconhece o tigre.

\begin{figure}[H]
  \centering
  \caption{Esquema de como uma CNN identificaria um tigre}
  \includegraphics[width=0.55\textwidth]{figuras/ann_exemplos/cnn_tigre.png}
  \label{fig:cnn_tigre}
  \legend{Fonte: Adaptado de \citeonline{james2023}}
\end{figure}

De modo geral, uma CNN é composta por duas estruturas principais: 
as \textit{camadas convolucionais}, responsáveis por extrair características relevantes dos dados, 
e as \textit{camadas de pooling}, que reduzem a dimensionalidade e mantêm apenas as informações mais significativas.

Para compreender o funcionamento de uma convolução, considere a seguinte matriz, representando uma imagem:

\[
\begin{bmatrix}
a & b & c \\
d & e & f \\
g & h & i \\
j & k & l \\
\end{bmatrix}
\]

e um filtro convolucional \(2 \times 2\):

\[
\begin{bmatrix}
\alpha & \beta \\
\gamma & \delta
\end{bmatrix}
\]

A operação de convolução consiste em multiplicar o filtro elemento a elemento com cada submatriz \(2 \times 2\) da imagem, 
somar os resultados e, em seguida, deslizar o filtro sobre a matriz. 
O resultado dessa operação é:

\[
\begin{bmatrix}
a\alpha + b\beta + d\gamma + e\delta & 
b\alpha + c\beta + e\gamma + f\delta \\[4pt]
d\alpha + e\beta + g\gamma + h\delta & 
e\alpha + f\beta + h\gamma + i\delta \\[4pt]
g\alpha + h\beta + j\gamma + k\delta & 
h\alpha + i\beta + k\gamma + l\delta \\
\end{bmatrix}
\]

O filtro atua como um detector de padrões locais: se uma submatriz se assemelha ao filtro, ela será destacada na saída. 
É importante notar que a convolução \textbf{não} é uma multiplicação matricial tradicional.

Após a convolução, aplica-se normalmente uma camada de \textit{pooling}, que resume as \textit{features} extraídas.
O \textit{max pooling}, por exemplo, seleciona o maior valor em cada submatriz \(2 \times 2\) não sobreposta:

\[
\text{Max pool} \quad
\begin{bmatrix}
1 & 2 & 5 & 3 \\
3 & 0 & 1 & 2 \\
2 & 1 & 3 & 4 \\
1 & 1 & 2 & 0
\end{bmatrix}
\rightarrow
\begin{bmatrix}
3 & 5 \\
2 & 4
\end{bmatrix}
\]

Na figura \ref{fig:cnn_ar_exemplo}, é ilustrada uma arquitetura típica de CNN.

\begin{figure}[H]
  \centering
  \caption{Arquitetura típica de uma CNN}
  \includegraphics[width=1.0\textwidth]{figuras/ann_exemplos/cnn_arquitetura.png}
  \label{fig:cnn_ar_exemplo}
  \legend{Fonte: Adaptado de \citeonline{james2023}}
\end{figure}

A entrada consiste em uma imagem tridimensional, na qual a terceira dimensão — denominada \textit{canal} — 
corresponde às cores no modelo RGB. 
A primeira camada convolucional transforma a imagem inicial em uma nova representação, 
por exemplo, de dimensão \((32, 32, 6)\), onde o número 6 corresponde à quantidade de filtros aplicados. 
Em seguida, uma camada de \textit{pooling} reduz essa dimensão para \((16, 16, 6)\). 
Novas camadas convolucionais e de \textit{pooling} podem ser adicionadas até que, ao final, 
os dados sejam achatados (\textit{flattened}) e passados para camadas totalmente conectadas (\textit{FC}) para a classificação.

De modo geral, cada camada convolucional aumenta a profundidade do canal (número de filtros), 
enquanto as camadas de \textit{pooling} reduzem as dimensões espaciais da imagem.

Por fim, conforme \citeonline{geron2022hands}, CNNs não se limitam a imagens: 
elas também podem ser aplicadas a sinais unidimensionais. 
Nesses casos, um filtro unidimensional é deslizado sobre o sinal, 
permitindo que a rede aprenda padrões locais (com tamanho máximo igual ao do filtro). 
Com múltiplos filtros, obtém-se uma sequência de \textit{features} que podem, em seguida, 
ser processadas por uma RNN, capturando dependências temporais mais longas.

\input{inputs/revisao_literatura}